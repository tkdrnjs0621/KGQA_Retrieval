{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from datasets import load_from_disk\n",
    "from functools import partial\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import timedelta\n",
    "import random\n",
    "random.seed(42)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"./datasets/Rog-webqsp_test_rr\")\n",
    "case_1 = dataset[0]\n",
    "case_2 = dataset[1]\n",
    "dataset = dataset.select(range(2,len(dataset)))\n",
    "dataset_1hop = dataset.filter(lambda x: x[\"gt_path_triplet\"]is not None and len(x[\"gt_path_triplet\"])==1)\n",
    "dataset_2hop = dataset.filter(lambda x: x[\"gt_path_triplet\"]is not None and len(x[\"gt_path_triplet\"])==2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['JaMarcus Russell', 'people.person.gender', 'Male'], ['JaMarcus Russell', 'people.person.parents', 'Bobby Lloyd'], ['JaMarcus Russell', 'people.person.nationality', 'United States of America'], ['JaMarcus Russell', 'people.person.ethnicity', 'African American'], ['JaMarcus Russell', 'common.topic.image', 'JaMarcus Russell at Falcons at Raiders 11-2-08'], ['JaMarcus Russell', 'people.person.parents', 'Zina L. Russell-Anderson'], ['JaMarcus Russell', 'people.person.places_lived', 'm.03phjs_'], ['JaMarcus Russell', 'sports.pro_athlete.sports_played_professionally', 'm.0c550qk']]\n",
      "[['JaMarcus Russell', 'people.person.place_of_birth', 'Mobile']]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0]['gn_path_triplet'])\n",
    "print(dataset[0]['gt_path_triplet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_messages_1hop(example,no_rag,zero_shot,n_negative,fs_case_1,fs_case_2):\n",
    "\n",
    "    q = example[\"question\"]\n",
    "    if(no_rag):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{q}?\"},\n",
    "        ]\n",
    "    else:\n",
    "        def get_knowledge_text(data):\n",
    "            triplet_list = data['gn_path_triplet'][:n_negative]+data['gt_path_triplet']\n",
    "            random.shuffle(triplet_list)\n",
    "            triplet_list = [\"(\"+\"->\".join(t)+\")\" for t in triplet_list]\n",
    "            txt=\"\\n\".join(triplet_list)\n",
    "            return txt\n",
    "        \n",
    "        txt = get_knowledge_text(example)\n",
    "        \n",
    "        if zero_shot:\n",
    "            messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers the user's question based on the user's knowledge.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"[Knowledge]\n",
    "{txt}\n",
    "[Question]\n",
    "{q}?\"\"\"}\n",
    "            ]      \n",
    "        else:\n",
    "            txt_1 = get_knowledge_text(fs_case_1)\n",
    "            q_1 = fs_case_1[\"question\"]\n",
    "            answer_1 = fs_case_1[\"a_entity\"][0]\n",
    "            txt_2 = get_knowledge_text(fs_case_2)\n",
    "            q_2 = fs_case_2[\"question\"]\n",
    "            answer_2 = fs_case_2[\"a_entity\"][0]\n",
    "\n",
    "            messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers the user's question based on the user's knowledge.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"[Knowledge]\n",
    "{txt_1}\n",
    "[Question]\n",
    "{q_1}?\"\"\"},\n",
    "            {\"role\": \"assistant\", \"content\": f\"The answer is {answer_1}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"[Knowledge]\n",
    "{txt_2}\n",
    "[Question]\n",
    "{q_2}?\"\"\"},\n",
    "            {\"role\": \"assistant\", \"content\": f\"The answer is {answer_2}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"[Knowledge]\n",
    "{txt}\n",
    "[Question]\n",
    "{q}?\"\"\"},\n",
    "            ]\n",
    "    example[\"messages\"] =messages\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d757fe19c64a029fb7d103457bb04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def set_file_handler(logger, path, level=logging.DEBUG, format=\"%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s\"):\n",
    "    os.makedirs(os.path.dirname(path + \"/run.log\"), exist_ok=True)\n",
    "    handler = logging.FileHandler(path + \"/run.log\")\n",
    "    handler.setLevel(level)\n",
    "    formatter = logging.Formatter(format)\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    \n",
    "logger = logging.getLogger(\"inference\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "set_file_handler(logger, \"./logs\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", torch_dtype=torch.bfloat16, device_map=\"auto\", attn_implementation=\"flash_attention_2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path, padding_side=\"left\")\n",
    "model.eval()\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862 27\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "def build_prompt(example, tokenizer):\n",
    "    prompt = tokenizer.apply_chat_template(example[\"messages\"], tokenize=False, add_generation_prompt=True)\n",
    "    prompt_tokens = len(tokenizer(prompt, add_special_tokens=False).input_ids)\n",
    "    return {\"prompt\": prompt, \"prompt_tokens\": prompt_tokens}\n",
    "\n",
    "\n",
    "def collate_fn(batch, tokenizer):\n",
    "    prompt = [example[\"prompt\"] for example in batch]\n",
    "    inputs = tokenizer(prompt, add_special_tokens=False, padding=True, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, dataloader, logger, log_every, **kwargs):\n",
    "    start = time.time()\n",
    "    output_ids = []\n",
    "    for i, inputs in tqdm(enumerate(dataloader, start=1)):\n",
    "        inputs = inputs.to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id, **kwargs)\n",
    "        output_ids.extend(outputs[:, inputs[\"input_ids\"].size(1) :].tolist())\n",
    "        if i % log_every == 0:\n",
    "            end = time.time()\n",
    "            elapsed = end - start\n",
    "            total = elapsed * (len(dataloader) / i)\n",
    "            logger.info(f\"Done {i}/{len(dataloader)} steps - {str(timedelta(seconds=int(elapsed)))}/{str(timedelta(seconds=int(total)))}.\")\n",
    "    return output_ids\n",
    "\n",
    "\n",
    "def decode(example, tokenizer, feature):\n",
    "    text = tokenizer.decode(example[feature + \"_ids\"], skip_special_tokens=True)\n",
    "    return {feature: text}\n",
    "\n",
    "dataset=dataset_1hop.map(partial(build_messages_1hop,no_rag=False,zero_shot=False,n_negative=2,fs_case_1=case_1,fs_case_2=case_2))\n",
    "dataset = dataset.map(partial(build_prompt, tokenizer=tokenizer), num_proc=16)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=16, collate_fn=partial(collate_fn, tokenizer=tokenizer), pin_memory=True)  # type: ignore\n",
    "print(len(dataset),len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [02:02,  4.53s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b6bed3c64d4b959d035aec60be916b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/862 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "output_ids = generate(model, tokenizer, dataloader, logger, 10, max_new_tokens=20, do_sample=True, temperature=0.5, top_k=50, top_p=0.5)\n",
    "\n",
    "dataset = dataset.add_column(\"model_answer_ids\", output_ids)  # type: ignore\n",
    "dataset = dataset.map(partial(decode, tokenizer=tokenizer, feature=\"model_answer\"), num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410 452\n"
     ]
    }
   ],
   "source": [
    "hit=0\n",
    "fail=0\n",
    "for row in dataset:\n",
    "    if row[\"a_entity\"][0] in row[\"model_answer\"]:\n",
    "        hit+=1\n",
    "    else:\n",
    "        fail+=1\n",
    "print(hit,fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': \"You are a helpful assistant that answers the user's question based on the user's knowledge.\", 'role': 'system'}, {'content': '[Knowledge]\\n(Jamaica->book.book_subject.works->Culture and Customs of Jamaica)\\n(Jamaica->location.country.languages_spoken->Jamaican English)\\n(Jamaica->location.country.official_language->Jamaican English)\\n[Question]\\nwhat does jamaican people speak?', 'role': 'user'}, {'content': \"The answer is ['Jamaican English', 'Jamaican Creole English Language']\", 'role': 'assistant'}, {'content': '[Knowledge]\\n(m.04j60kc->government.government_position_held.office_position_or_title->United States Representative)\\n(James K. Polk->common.topic.image->James K. Polk)\\n(James K. Polk->government.politician.government_positions_held->m.04j60kc)\\n(James K. Polk->common.topic.image->James K. Polk)\\n[Question]\\nwhat did james k polk do before he was president?', 'role': 'user'}, {'content': \"The answer is ['United States Representative', 'Governor of Tennessee', 'Speaker of the United States House of Representatives']\", 'role': 'assistant'}, {'content': '[Knowledge]\\n(JaMarcus Russell->people.person.place_of_birth->Mobile)\\n(JaMarcus Russell->people.person.parents->Bobby Lloyd)\\n(JaMarcus Russell->people.person.gender->Male)\\n[Question]\\nwhere is jamarcus russell from?', 'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model_id,\n",
    "#     model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "# ]\n",
    "\n",
    "# outputs = pipeline(\n",
    "#     messages,\n",
    "#     max_new_tokens=256,\n",
    "# )\n",
    "# print(outputs[0][\"generated_text\"][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_241",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
